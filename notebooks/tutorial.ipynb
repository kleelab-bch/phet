{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtype Detection using PHet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mr:\\GeneAnalysis\\phet\\notebooks\\tutorial.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/r%3A/GeneAnalysis/phet/notebooks/tutorial.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscanpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msc\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/r%3A/GeneAnalysis/phet/notebooks/tutorial.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/r%3A/GeneAnalysis/phet/notebooks/tutorial.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mphet\u001b[39;00m \u001b[39mimport\u001b[39;00m PHeT\n\u001b[0;32m      <a href='vscode-notebook-cell:/r%3A/GeneAnalysis/phet/notebooks/tutorial.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutility\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile_path\u001b[39;00m \u001b[39mimport\u001b[39;00m DATASET_PATH, RESULT_PATH\n\u001b[0;32m      <a href='vscode-notebook-cell:/r%3A/GeneAnalysis/phet/notebooks/tutorial.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutility\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplot_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_umap, plot_barplot\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "from src.model.phet import PHeT\n",
    "from src.utility.file_path import DATASET_PATH, RESULT_PATH\n",
    "from src.utility.plot_utils import plot_umap, plot_barplot\n",
    "from src.utility.utils import comparative_score\n",
    "from src.utility.utils import sort_features, significant_features\n",
    "\n",
    "sns.set_theme(style=\"white\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Hyperparamters\n",
    "\n",
    "Two files will be downloaed ontologies and associations. Mapping Genes IDs to namedtuples consist of symbols and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "pvalue = 0.01\n",
    "sort_by_pvalue = True\n",
    "topKfeatures = 100\n",
    "plot_topKfeatures = False\n",
    "if not sort_by_pvalue:\n",
    "    plot_topKfeatures = True\n",
    "is_filter = True\n",
    "num_neighbors = 5\n",
    "max_clusters = 10\n",
    "feature_metric = \"f1\"\n",
    "cluster_type = \"spectral\"\n",
    "export_spring = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # descriptions of the data\n",
    "    file_name = \"baron_1\"\n",
    "    suptitle_name = \"Baron\"\n",
    "    control_name = \"0\"\n",
    "    case_name = \"1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Exprssion, classes, subtypes, donors, timepoints Files\n",
    "    expression_file_name = file_name + \"_matrix.mtx\"\n",
    "    features_file_name = file_name + \"_feature_names.csv\"\n",
    "    markers_file = file_name + \"_markers.csv\"\n",
    "    classes_file_name = file_name + \"_classes.csv\"\n",
    "    subtypes_file = file_name + \"_types.csv\"\n",
    "    differential_features_file = file_name + \"_limma_features.csv\"\n",
    "    donors_file = file_name + \"_donors.csv\"\n",
    "    timepoints_file = file_name + \"_timepoints.csv\"\n",
    "\n",
    "    # Load subtypes file\n",
    "    subtypes = pd.read_csv(os.path.join(DATASET_PATH, subtypes_file), sep=',').dropna(axis=1)\n",
    "    subtypes = [str(item[0]).lower() for item in subtypes.values.tolist()]\n",
    "    num_clusters = len(np.unique(subtypes))\n",
    "    donors = []\n",
    "    if os.path.exists(os.path.join(DATASET_PATH, donors_file)):\n",
    "        donors = pd.read_csv(os.path.join(DATASET_PATH, donors_file), sep=',').dropna(axis=1)\n",
    "        donors = [str(item[0]).lower() for item in donors.values.tolist()]\n",
    "    timepoints = []\n",
    "    if os.path.exists(os.path.join(DATASET_PATH, timepoints_file)):\n",
    "        timepoints = pd.read_csv(os.path.join(DATASET_PATH, timepoints_file), sep=',').dropna(axis=1)\n",
    "        timepoints = [str(item[0]).lower() for item in timepoints.values.tolist()]\n",
    "\n",
    "    # Load features, expression, and class data\n",
    "    features_name = pd.read_csv(os.path.join(DATASET_PATH, features_file_name), sep=',')\n",
    "    features_name = features_name[\"features\"].to_list()\n",
    "    y = pd.read_csv(os.path.join(DATASET_PATH, classes_file_name), sep=',')\n",
    "    y = y[\"classes\"].to_numpy()\n",
    "    X = sc.read_mtx(os.path.join(DATASET_PATH, expression_file_name))\n",
    "    X = X.to_df().to_numpy()\n",
    "    np.nan_to_num(X, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Filter data \n",
    "    num_examples, num_features = X.shape\n",
    "    if is_filter:\n",
    "        example_sums = np.absolute(X).sum(1)\n",
    "        examples_ids = np.where(example_sums > int(0.01 * num_features))[0]\n",
    "        X = X[examples_ids]\n",
    "        y = y[examples_ids]\n",
    "        subtypes = np.array(subtypes)[examples_ids].tolist()\n",
    "        if len(donors) != 0:\n",
    "            donors = np.array(donors)[examples_ids].tolist()\n",
    "        if len(timepoints) != 0:\n",
    "            timepoints = np.array(timepoints)[examples_ids].tolist()\n",
    "        num_examples, num_features = X.shape\n",
    "        del example_sums, examples_ids\n",
    "        temp = np.absolute(X)\n",
    "        temp = (temp * 1e6) / temp.sum(axis=1).reshape((num_examples, 1))\n",
    "        temp[temp > 1] = 1\n",
    "        temp[temp != 1] = 0\n",
    "        feature_sums = temp.sum(0)\n",
    "        del temp\n",
    "        feature_ids = np.where(feature_sums > int(0.01 * num_examples))[0]\n",
    "        features_name = np.array(features_name)[feature_ids].tolist()\n",
    "        X = X[:, feature_ids]\n",
    "        num_examples, num_features = X.shape\n",
    "        del feature_sums\n",
    "\n",
    "    # Save subtypes for SPRING\n",
    "    if export_spring:\n",
    "        groups = []\n",
    "        groups.append([\"subtypes\"] + subtypes)\n",
    "        if len(donors) != 0:\n",
    "            groups.append([\"donors\"] + donors)\n",
    "        if len(timepoints) != 0:\n",
    "            groups.append([\"timepoints\"] + timepoints)\n",
    "        df = pd.DataFrame(groups)\n",
    "        df.to_csv(os.path.join(RESULT_PATH, file_name + \"_groups.csv\"), sep=',',\n",
    "                  index=False, header=False)\n",
    "        del df\n",
    "\n",
    "    # Load up/down regulated features\n",
    "    top_features_true = -1\n",
    "    if os.path.exists(os.path.join(DATASET_PATH, markers_file)):\n",
    "        top_features_true = pd.read_csv(os.path.join(DATASET_PATH, markers_file)).replace(np.nan, -1)\n",
    "        top_features_true = list(set([item for item in top_features_true.to_numpy().flatten() if item != -1]))\n",
    "        top_features_true = [1 if feature in top_features_true else 0 for idx, feature in enumerate(features_name)]\n",
    "        topKfeatures = sum(top_features_true)\n",
    "    elif os.path.exists(os.path.join(DATASET_PATH, differential_features_file)):\n",
    "        top_features_true = pd.read_csv(os.path.join(DATASET_PATH, differential_features_file), sep=',',\n",
    "                                        index_col=\"ID\")\n",
    "        temp = [feature for feature in top_features_true.index.to_list() if str(feature) in features_name]\n",
    "        if top_features_true.shape[1] > 0:\n",
    "            top_features_true = top_features_true.loc[temp]\n",
    "            temp = top_features_true[top_features_true[\"adj.P.Val\"] <= pvalue]\n",
    "            if temp.shape[0] < topKfeatures:\n",
    "                temp = top_features_true[:topKfeatures - 1]\n",
    "                if sort_by_pvalue and temp.shape[0] == 0:\n",
    "                    plot_topKfeatures = True\n",
    "            top_features_true = [str(feature_idx) for feature_idx in temp.index.to_list()[:topKfeatures]]\n",
    "        else:\n",
    "            top_features_true = temp\n",
    "            topKfeatures = len(top_features_true)\n",
    "        top_features_true = [1 if feature in top_features_true else 0 for idx, feature in enumerate(features_name)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    print(\"## Perform experimental studies using {0} data...\".format(file_name))\n",
    "    print(\"\\t >> Sample size: {0}; Feature size: {1}; Subtype size: {2}\".format(X.shape[0], X.shape[1],\n",
    "                                                                                len(np.unique(subtypes))))\n",
    "    current_progress = 1\n",
    "    total_progress = len(methods)\n",
    "    methods_dict = dict()\n",
    "\n",
    "    print(\"\\t >> Progress: {0:.4f}%; Method: {1:20}\".format((current_progress / total_progress) * 100,\n",
    "                                                            methods[0]))\n",
    "    estimator = PHeT(normalize=\"zscore\", iqr_range=(25, 75), num_subsamples=1000, calculate_deltaiqr=True,\n",
    "                     calculate_deltahvf=False, calculate_fisher=True, calculate_profile=True, bin_KS_pvalues=False,\n",
    "                     feature_weight=[0.4, 0.3, 0.2, 0.1], weight_range=[0.2, 0.4, 0.8])\n",
    "    df = estimator.fit_predict(X=X, y=y, control_class=0, case_class=1)\n",
    "    methods_dict.update({methods[0]: df})\n",
    "    current_progress += 1\n",
    "\n",
    "    if sort_by_pvalue:\n",
    "        print(\"## Sort features by the cut-off {0:.2f} p-value...\".format(pvalue))\n",
    "    else:\n",
    "        print(\"## Sort features by the score statistic...\".format())\n",
    "    for method_idx, item in enumerate(methods_dict.items()):\n",
    "        method_name, df = item\n",
    "        method_name = methods[method_idx]\n",
    "        save_name = methods_save_name[method_idx]\n",
    "        if sort_by_pvalue:\n",
    "            temp = significant_features(X=df, features_name=features_name, pvalue=pvalue,\n",
    "                                        X_map=None, map_genes=False, ttest=False)\n",
    "        else:\n",
    "            temp = sort_features(X=df, features_name=features_name, X_map=None,\n",
    "                                 map_genes=False, ttest=False)\n",
    "        methods_dict[method_name] = temp\n",
    "    del df\n",
    "\n",
    "    if top_features_true != -1:\n",
    "        print(\"## Scoring results using known regulated features...\")\n",
    "        selected_regulated_features = topKfeatures\n",
    "        temp = np.sum(top_features_true)\n",
    "        if selected_regulated_features > temp:\n",
    "            selected_regulated_features = temp\n",
    "        print(\"\\t >> Number of up/down regulated features: {0}\".format(selected_regulated_features))\n",
    "        list_scores = list()\n",
    "        for method_idx, item in enumerate(methods_dict.items()):\n",
    "            if method_idx + 1 == len(methods):\n",
    "                print(\"\\t\\t--> Progress: {0:.4f}%; Method: {1:20}\".format(((method_idx + 1) / len(methods)) * 100,\n",
    "                                                                          methods[method_idx]))\n",
    "            else:\n",
    "                print(\"\\t\\t--> Progress: {0:.4f}%; Method: {1:20}\".format((method_idx / len(methods)) * 100,\n",
    "                                                                          methods[method_idx]), end=\"\\r\")\n",
    "            method_name, df = item\n",
    "            temp = [idx for idx, feature in enumerate(features_name)\n",
    "                    if feature in df['features'][:selected_regulated_features].tolist()]\n",
    "            top_features_pred = np.zeros((len(top_features_true)))\n",
    "            top_features_pred[temp] = 1\n",
    "            score = comparative_score(pred_features=top_features_pred, true_features=top_features_true,\n",
    "                                      metric=feature_metric)\n",
    "            list_scores.append(score)\n",
    "\n",
    "        df = pd.DataFrame(list_scores, columns=[\"Scores\"], index=methods)\n",
    "        df.to_csv(path_or_buf=os.path.join(RESULT_PATH, file_name + \"_features_scores.csv\"), sep=\",\")\n",
    "        print(\"## Plot barplot using the top {0} features...\".format(topKfeatures))\n",
    "        plot_barplot(X=list_scores, methods_name=methods, metric=\"f1\", suptitle=suptitle_name,\n",
    "                     file_name=file_name, save_path=RESULT_PATH)\n",
    "\n",
    "    temp = np.copy(y)\n",
    "    temp = temp.astype(str)\n",
    "    temp[np.where(y == 0)[0]] = control_name\n",
    "    temp[np.where(y == 1)[0]] = case_name\n",
    "    y = temp\n",
    "    list_scores = list()\n",
    "    score = 0\n",
    "    print(\"## Plot UMAP using all features ({0})...\".format(num_features))\n",
    "    score = plot_umap(X=X, y=y, subtypes=subtypes, features_name=features_name, num_features=num_features,\n",
    "                      standardize=True, num_neighbors=num_neighbors, min_dist=0, perform_cluster=True,\n",
    "                      cluster_type=cluster_type, num_clusters=num_clusters, max_clusters=max_clusters,\n",
    "                      apply_hungarian=False, heatmap_plot=False, num_jobs=num_jobs, suptitle=suptitle_name + \"\\nAll\",\n",
    "                      file_name=file_name + \"_all\", save_path=RESULT_PATH)\n",
    "    list_scores.append(score)\n",
    "    if top_features_true != -1:\n",
    "        print(\"## Plot UMAP using marker features ({0})...\".format(sum(top_features_true)))\n",
    "        temp = np.where(np.array(top_features_true) == 1)[0]\n",
    "        score = plot_umap(X=X[:, temp], y=y, subtypes=subtypes, features_name=features_name, num_features=temp.shape[0],\n",
    "                          standardize=True, num_neighbors=num_neighbors, min_dist=0, perform_cluster=True,\n",
    "                          cluster_type=cluster_type, num_clusters=num_clusters, max_clusters=max_clusters,\n",
    "                          apply_hungarian=False, heatmap_plot=False, num_jobs=num_jobs,\n",
    "                          suptitle=suptitle_name + \"\\nMarkers\",\n",
    "                          file_name=file_name + \"_markers\", save_path=RESULT_PATH)\n",
    "        list_scores.append(score)\n",
    "\n",
    "    if plot_topKfeatures:\n",
    "        print(\"## Plot UMAP using the top {0} features...\".format(topKfeatures))\n",
    "    else:\n",
    "        print(\"## Plot UMAP using the top features for each method...\")\n",
    "    for method_idx, item in enumerate(methods_dict.items()):\n",
    "        method_name, df = item\n",
    "        method_name = methods[method_idx]\n",
    "        save_name = methods_save_name[method_idx]\n",
    "        if total_progress == method_idx + 1:\n",
    "            print(\"\\t >> Progress: {0:.4f}%; Method: {1:20}\".format(((method_idx + 1) / total_progress) * 100,\n",
    "                                                                    method_name))\n",
    "        else:\n",
    "            print(\"\\t >> Progress: {0:.4f}%; Method: {1:20}\".format(((method_idx + 1) / total_progress) * 100,\n",
    "                                                                    method_name), end=\"\\r\")\n",
    "        if plot_topKfeatures:\n",
    "            temp = [idx for idx, feature in enumerate(features_name) if\n",
    "                    feature in df['features'].tolist()[:topKfeatures]]\n",
    "            temp_feature = [feature for idx, feature in enumerate(features_name) if\n",
    "                            feature in df['features'].tolist()[:topKfeatures]]\n",
    "        else:\n",
    "            temp = [idx for idx, feature in enumerate(features_name) if feature in df['features'].tolist()]\n",
    "            temp_feature = [feature for idx, feature in enumerate(features_name) if feature in df['features'].tolist()]\n",
    "        num_features = len(temp)\n",
    "        score = plot_umap(X=X[:, temp], y=y, subtypes=subtypes, features_name=temp_feature, num_features=num_features,\n",
    "                          standardize=True, num_neighbors=num_neighbors, min_dist=0.0, perform_cluster=True,\n",
    "                          cluster_type=cluster_type, num_clusters=num_clusters, max_clusters=max_clusters,\n",
    "                          apply_hungarian=False, heatmap_plot=False, num_jobs=num_jobs,\n",
    "                          suptitle=suptitle_name + \"\\n\" + method_name, file_name=file_name + \"_\" + save_name.lower(),\n",
    "                          save_path=RESULT_PATH)\n",
    "        df = pd.DataFrame(temp_feature, columns=[\"features\"])\n",
    "        df.to_csv(os.path.join(RESULT_PATH, file_name + \"_\" + save_name.lower() + \"_features.csv\"),\n",
    "                  sep=',', index=False, header=False)\n",
    "        if export_spring:\n",
    "            df = pd.DataFrame(X[:, temp])\n",
    "            df.to_csv(path_or_buf=os.path.join(RESULT_PATH, file_name + \"_\" + save_name.lower() + \"_expression.csv\"),\n",
    "                      sep=\",\", index=False, header=False)\n",
    "        del df\n",
    "        list_scores.append(score)\n",
    "    index = [\"All\"]\n",
    "    if top_features_true != -1:\n",
    "        index += [\"Markers\"]\n",
    "    df = pd.DataFrame(list_scores, columns=[\"Scores\"], index=index + methods)\n",
    "    df.to_csv(path_or_buf=os.path.join(RESULT_PATH, file_name + \"_cluster_quality.csv\"), sep=\",\")\n",
    "\n",
    "    print(\"## Plot barplot using to demonstrate clustering accuracy...\".format(topKfeatures))\n",
    "    plot_barplot(X=list_scores, methods_name=index + methods, metric=\"ari\",\n",
    "                 suptitle=suptitle_name, file_name=file_name, save_path=RESULT_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a GOEA object\n",
    "The GOEA object holds the Ontologies, Associations, and background.    \n",
    "Numerous studies can then be run withough needing to re-load the above items.    \n",
    "In this case, we only run one GOEA.    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GOEA on Ionocytes using all Mouse Genes\n",
    "\n",
    "In this case study, we run GOEA for ~400 genes from PHet's results from ionocytes   \n",
    "and unknown populations of ionocytes. We do two tests to search for functions for   \n",
    "iocnocytes enriched terms and unknown populations enriched terms. In this example,  \n",
    "we choose to keep only the significant results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the permutation based significant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features = os.path.join(os.getcwd(), \"permutaion_ionocytes_features.csv\")\n",
    "significant_features = pd.read_csv(significant_features, sep=',', header=None)[0].tolist()\n",
    "significant_features = [item.capitalize() for item in significant_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load enriched terms for both ionocytes and unknown populations using SPRING results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = os.path.join(os.getcwd(), \"enriched_terms_ionocytes.txt\")\n",
    "features = pd.read_csv(features, sep='\\t', header=None)\n",
    "features.columns = [\"Features\", \"Scores\"]\n",
    "ionocytes_features = []\n",
    "uk_features = []\n",
    "for f in features.iterrows():\n",
    "    gene = f[1][0]\n",
    "    scores = f[1][1]\n",
    "    if gene.lower().startswith(\"rp\"):\n",
    "          continue\n",
    "    if scores > 0:\n",
    "         ionocytes_features.append(gene)\n",
    "    elif scores < 0:\n",
    "         uk_features.append(gene)\n",
    "uk_features = uk_features[::-1]\n",
    "\n",
    "ionocytes_features = ionocytes_features[:top_features]\n",
    "uk_features = uk_features[:top_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, use features generated from Scanpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
